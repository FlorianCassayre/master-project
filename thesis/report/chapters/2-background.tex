\section{Background}
\label{sec:background}

Attempts at formalizing logic and mathematics are numerous, and one can observe that successful systems are often the result of an iterative process \cite{Paulson2019, Harrison2014, Asperti2007}. In this section we describe the history of proof assistants, and we present our past contributions to the field.

\subsection{Overview}

Historically, proof assistants consisted mainly on automating the process of demonstrating assertions rather than providing tools to assist the user in doing so; which is objectively a simpler task \cite{Harrison2014}. It is only later that designs have leaned towards user-centered systems with ideas such as the Edinburgh LCF, and subsequent work on metalogic and tactics \cite{Milner1984}. Usability gradually improved, as progress was made in the areas of higher-order logic, type systems and unification \cite{Paulson2019}. The proof assistants that are currently available vary in designs, however different systems often share a set of similar features. Coq and Isabelle/HOL are two examples of widely adopted systems that have a vast repertoire of proven theorems \cite{Yushkovskiy2018}. Both support forward and backward reasoning, and are expressive enough to write procedures such as tactics. Their main differences boil down to the underlying theory and the implementation of their verifier \cite{Barras1999, Wenzel2021}: Coq is based on the calculus of constructions, while Isabelle/HOL relies on higher-order logic.

\subsection{LCF-style framework}

In 2020 I worked on designing a LCF-style framework in Scala that used the Von Neumann-Bernays-GÃ¶del set theory (NBG) as a foundation \cite{Cassayre2020} and attempted to formalize the proofs displayed in a logic textbook \cite{Mendelson2015}. This project allowed me to explore different areas of formal methods and theorem proving such as the representation of proofs and tableaux solving strategies. The particularities of that framework were the possibility to recover the value of a formula from its type and the correspondence between lambda expressions and logical implications. In the lines of the \textit{Curry-Howard correspondence}, this meant that soundness could be enforced by the type checker alone. Unavoidably, this feature gets in the way when working with higher-order rules (e.g. induction) and automated strategies (e.g. tableaux). It also demonstrated that the raw LCF-style restricted the expressiveness of proofs, and that to provide more flexibility an extra layer would be needed.

LISA uses the Zermelo-Fraenkel set theory (ZF, and more specifically ZFC) instead of NBG. Both axiomatizations are similar, the main difference being that NBG makes the distinction between sets and proper classes. NBG has been (independently) proven to be a conservative extension of ZFC, meaning that neither is stronger than the other. Furthermore, LISA uses sequent calculus in its deductive system, while that framework had formulas as first-class citizens.

\subsection{LISA}

Late 2021 before starting working on this thesis, I contributed to the design of LISA, notably by maintaining the codebase, reviewing and testing the proof checker, and exploring the design of third-party tools for LISA. This LISA project then ramified itself into several sub-projects, including this very thesis. The topic of projects lead by other persons include: formalization of other theories such as Peano's arithmetic, proof space exploration using deep learning methods, translation of natural language sentences into formal statements, and interoperability with other tools, for example Stainless \cite{Kuncak2021}. During the period of the thesis I continued to occasionally work on LISA as it proceeded further in its maturation process. Some components and ideas designed in this front-end framework were integrated in LISA. Eventually we are aiming to integrate the full framework proposed in this thesis as a module of LISA.
